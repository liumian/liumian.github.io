---
layout: post
title: CUDA多GPU编程系列一
description: multi-GPU  programming   
category: blog
---

##概述
前言及声明

- GPU通信（单节点与多节点）
- 多GPU、streams、events
- 两个API


##为什么需要多GPU？
<ul>
	<li>进一步提高加速比</li>
	<li>工作集(working set)已经超过了一个GPU的内存</li>
</ul>


##GPU通信(Single Host, Multiple GPUs)

###单个CPU线程管理多GPU
<ul> 
    <li>CUDA指令--针对当前的GPU
        <ul>例外：peer-to-peer 内存拷贝</ul></li> 
	<li>cudaSetDevice()设置当前GPU</li>
	<li>执行异步指令(kernels,memcopies)当前的GPU也是能被改变的
		<ul>如下代码可以让这两个GPU并发执行</ul>
	</li>
</ul>
<pre>    
    cudaSetDevice( 0 ); 
	kernel<<<...>>>(...); 
	cudaMemcpyAsync(...); 
	cudaSetDevice( 1 ); 
	kernel<<<...>>>(...); 
</pre>	
###统一地址(CUDA4.0及以后)
<ul> 
    <li>CPU和GPU使用统一虚拟地址空间
        <ul>
           每个CPU/GPU都有属于它自己的一段虚拟地址(VA)空间
           <li>
             Drive/GPU可以根据地址判断出数据在哪里
           </li>
           <li>
             每次分配都是在单个设备上(数组不能跨GPU)
           </li>
        </ul>
        <ul>
           配置需要
           <li>
             带有TCC驱动的64位Linux或者64位Windows系统
           </li>
           <li>
            Fermi或更新的GPU架构(计算能力要在2.0以上)
           </li>
           <li>
             CUDA4.0以上
           </li>
        </ul>
    </li> 
	<li> GPU可以引用的指针
		<ul>
			<li>另一个GPU上的地址</li>
			<li>主机上的地址(CPU)</li>
		</ul>
	</li>
</ul>


###UVA和多GPU编程
- 两个方面
  + Peer-to-peer(P2P)的内存拷贝
  + 访问另一个GPU的地址
- 上面的两个方面 都要获得访问的允许
  + `cudaDeviceEnablePeerAccess( peer_device, 0 )`
    - 可以让当前GPU获得访问peer_device GPU地址的权利
 + `cudaDeviceCanAccessPeer( &amp;accessible, dev_X, dev_Y )`
   - 检查是否dev_X可以访问dev_Y的内存
   - 返回值为0/1
   w- 如果不能访问(即返回0)可能的原因有 某个GPU是Fermi架构之前的、GPU连在了母版上不同的IOH芯片上(QPI和PCI-E协议在P2P中不支持)

###Peer-to-peer内存拷贝
- `cudaMemcpyPeerAsync( void* dst_addr, int dst_dev, void* src_addr, int src_dev,  size_t num_bytes, cudaStream_t stream )`

  + 在两个设备之间拷贝内存
  + 源GPU的DMA引擎执行了这次拷贝
- 如果允许peer-access
  + 通过最近的PCI-E传输数据
  + 完全不经过CPU内存
- 如果不允许peer-access
  + CUDA drive通过CPU内存传输

###一个实例：4个GPU 从左到右传输法
<a href="" title="GPU" target="_blank"><img src="/images/GPU/GPU_0.png" alt="GPU"></a>
  那么我们如何实现这四个GPU之间的数据交换呢？
  第一步：send "right"/receive from "left"
<a href="" title="GPU" target="_blank"><img src="/images/GPU/GPU_1.png" alt="GPU"></a> 
  第二步：send "left"/ receive from "right"
<a href="" title="GPU" target="_blank"><img src="/images/GPU/GPU_2.png" alt="GPU"></a>
  这步中三次传输是并发的，效果惊人，吞吐最大达到15GB/s

  
<pre>
for( int i=0; i &lt;num_gpus-1; i++ ) // "right" stage 
cudaMemcpyPeerAsync( d_a[i+1], gpu[i+1], d_a[i], gpu[i], num_bytes, stream[i] ); 
for( int i=0; i &lt;num_gpus; i++ ) 
cudaStreamSynchronize( stream[i] ); 
for( int i=1; i &lt;num_gpus; i++ ) // "left" stage 
cudaMemcpyPeerAsync(d_b[i-1], gpu[i-1], d_b[i], gpu[i], num_bytes, stream[i] );
</pre> 


##GPU通信(Multiple Host, Multiple GPUs) 

###不同节点间GPU的通信
- 需要局域网间的通信  
  ——直到现在仍然需要先把数据传到host
- 传送数据的步骤
  + GPU到CPU
  + CPU通过网络转换（举例：MPI_Sendrecv）
  + CPU到GPU
- 如果每个节点也有多GPU
  + 节点内部使用P2P,节点外用network
- 代码的模式
<pre>
    cudaMemcpyAsync( ..., stream_halo[i] ); 
 	cudaStreamSynchronize( stream_halo[i] ); 
 	MPI_Sendrecv( ... ); 
 	cudaMemcpyAsync( ..., stream_halo[i] ); 
</pre>
##多GPU,stream,events

   [参考之前做的pdf](/images/GPU/gpu.pdf)

##两个有用的API
###cudalpc* API
- 可以说cuda5.0之后出现的GPUdirect 3.0版的核心就是节点内GPU的RDMA
- 看到似乎需要Kepler架构，需要研究
- 代码演示如下  发送方：process A，接收方：process B

<pre>
    //Process A: 
	cudaIpcMemHandle_t   handle_a; 
	cudaIpcGetMemHandle( &amp;handle_a, (void*)sender_dev_buffer ); 
	//Process B: 
	cudaIpcMemHandle_t   handle_b;
	MPI_Irecv(handle_b,sizeof(cudaIpcMemHandle_t),...)
	cudaIpcOpenMemHandle( b, handle_b,   cudaIpcMemLazyEnablePeerAccess ); 
	// 这里使用设备缓冲(device buffer)b
	cudaIpcCloseMemHandle( handle_b ); 
</pre>
###其实就是刚才的MPI
